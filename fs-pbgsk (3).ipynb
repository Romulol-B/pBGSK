{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":56537,"databundleVersionId":8877088,"sourceType":"competition"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import polars as pl\nimport numpy as np\nimport seaborn as sns\nimport sklearn as sk\nimport random\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom joblib import Parallel, delayed\nfrom sklearn.metrics import r2_score\nfrom xgboost import XGBRegressor\nimport plotly.graph_objs as go\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"leap_raw_df = pl.scan_csv(f'/kaggle/input/leap-atmospheric-physics-ai-climsim/train.csv')\nfetch_size =1000\ninput_columns = leap_raw_df.columns[1:557]\noutput_columns = leap_raw_df.columns[557:924]\nX = leap_raw_df.select(pl.col(input_columns)).fetch(fetch_size)\nY = leap_raw_df.select(pl.col(output_columns)).fetch(fetch_size)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import FunctionTransformer\nconstant_epson = 1\nconstant_epson_r = 1\ndef log_transform(x):\n    return (np.log1p(x+constant_epson) +constant_epson_r)\n\n# Convert Polars DataFrame to NumPy array\nX_np = X.to_numpy()\nY_np = Y.to_numpy()\n\ntransformer_X = StandardScaler().fit(X_np)\ntransformer_Y = StandardScaler().fit(Y_np)\n\nX_transformed = transformer_X.transform(X_np)\nY_transformed = transformer_Y.transform(Y_np)\n\n\n\n# Log\n#transformer =FunctionTransformer(log_transform)\n#X_transformado=transformer.transform(X_np)\n#Y_transformado=transformer.transform(Y_np)\ndel X_np,Y_np,X,Y\nprint(X_transformed.dtype)\nprint(Y_transformed.dtype)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_transformed.shape[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def grafico_performace(feature_weighted_fitness):\n    \n    # Create an interactive plot\n    features = list(range(1, len(feature_weighted_fitness) + 1))\n    trace = go.Scatter(\n        x=features,\n        y=feature_weighted_fitness,\n        mode='lines+markers',\n        marker=dict(size=10, color='red'),\n        line=dict(color='blue', width=2),\n        hoverinfo='x+y+text',\n        text=['Feature {}'.format(i) for i in features]\n    )\n\n    layout = go.Layout(\n        title='Feature Frequency*Fitness Over Generations',\n        xaxis=dict(title='Feature'),\n        yaxis=dict(title='Frequency*Fitness'),\n        hovermode='closest'\n    )\n\n    fig = go.Figure(data=[trace], layout=layout)\n    fig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cupy as cp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fitness_function(solution, X_transformed =X_transformed, Y_transformed=Y_transformed):\n    from statistics import mean\n    selected_features = np.where(solution == 1)[0]\n    if len(selected_features) == 0:\n        return 0\n    X_selected = X_transformed[:, selected_features]\n    X_train, X_val, Y_train, Y_val = train_test_split(X_selected, Y_transformed, test_size=0.2, random_state=42)\n    X_train=cp.asarray(X_train)\n    X_val=cp.asarray(X_val)\n    Y_train=cp.asarray(Y_train)\n    Y_val=cp.asarray(Y_val)\n    r2_values = []\n    \n    for i in range(361,362):\n        model = XGBRegressor(\n            n_estimators= 1150,\n            max_depth=56,\n            learning_rate=0.003,\n            colsample_bytree=0.9,\n            subsample=0.8,\n            min_child_weight=1,\n            random_state=42,\n            objective='reg:squarederror',\n            n_jobs=-1,  # Use all available cores\n            tree_method='hist',\n            device ='cuda',\n            early_stopping_rounds = 6\n        )\n        zeros =0\n        model.fit(\n            X_train, Y_train[:, i],\n            eval_set=[(X_val, Y_val[:, i])],\n            verbose=False\n        )\n        y_pred = model.predict(X_val)\n        r2 = r2_score(Y_val[:, i].get(), y_pred)\n        if r2 == 0:\n            r2 = 1\n        r2_values.append(r2)\n    punicao = 3.96e-4  # 0.1 R^2 para metade do dataset\n    print(f\"individo incial com r^2 de {mean(r2_values)}, e com score punido de {mean(r2_values) - punicao * X_selected.shape[1]}, dimens√£o={X_selected.shape[1]}\")\n    return -mean(r2_values) +punicao*X_selected.shape[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# k_factor function\ndef k_factor(kf):\n    return 1 if random.random() < kf else 0\n\ndef gaining_phase(population, fitness_tuples, kf):\n    new_population = population.copy()\n    dimensao = 60\n    f_unidimensionais = 16\n    jota = 355  # Starting index for unidimensional features\n\n    for i in range(len(population)):\n        # Find the junior_xt_position\n        junior_xt_position = int(next(idx for idx, fit in fitness_tuples if idx == i))\n        junior_xt_0 = int((junior_xt_position - 1) % len(fitness_tuples))\n        junior_xt_1 = int((junior_xt_position + 1) % len(fitness_tuples))\n\n        partner = int(random.choice(range(len(population))))\n\n        if (fitness_tuples[junior_xt_position][1] > fitness_tuples[partner][1])and k_factor(kf):\n            for j in range(25):\n                if j < 6:\n                    start = j * dimensao\n                    end = start + dimensao\n                elif j > 21:\n                    start = (360 + f_unidimensionais) +(j-22)*dimensao\n                    end = start + dimensao\n                else:\n                    start = (jota + j) \n                    end = start+1                  \n                    \n                new_population[i][start:end] = (\n                    new_population[i][start:end] +\n                    k_factor(kf) * (\n                        population[partner][start:end] - \n                        new_population[i][start:end] + \n                        population[fitness_tuples[junior_xt_0][0]][start:end] - \n                        population[int(fitness_tuples[junior_xt_1][0])][start:end]\n                    )\n                )\n                # Ensure the new solution values are between 0 and 1\n                new_population[i][start:end] = np.clip(new_population[i][start:end], 0, 1)\n                \n            for j in range(25):\n                if j < 6:\n                    start = j * dimensao\n                    end = start + dimensao\n                elif j > 21:\n                    start = (360 + f_unidimensionais) +(j-22)*dimensao\n                    end = start + dimensao\n                else:\n                    start = (jota + j) \n                    end = start+1  \n\n                new_population[i][start:end] = (\n                    new_population[i][start:end] +\n                    k_factor(kf) * (\n                        new_population[i][start:end] - \n                        population[partner][start:end] + \n                        population[fitness_tuples[junior_xt_0][0]][start:end] - \n                        population[int(fitness_tuples[junior_xt_1][0])][start:end]\n                    )\n                )\n                new_population[i][start:end] = np.clip(new_population[i][start:end], 0, 1)\n\n    return new_population\n\n\n\n# Senior gaining phase function\ndef senior_gaining_phase(population, fitness_tuples, kf, p=0.1):\n    num_individuals = len(population)\n    num_seniors = int(num_individuals * p)\n    num_juniors = num_seniors\n    num_middles = num_individuals - num_seniors - num_juniors\n\n    # Ensure at least one senior, middle, and junior\n    if num_seniors == 0:\n        num_seniors = 1\n    if num_juniors == 0:\n        num_juniors = 1\n    if num_middles < 0:\n        num_middles = num_individuals - 2\n\n    senior_indices = [idx for idx, fit in fitness_tuples[:num_seniors]]\n    middle_indices = [idx for idx, fit in fitness_tuples[num_seniors:num_seniors + num_middles]]\n    junior_indices = [idx for idx, fit in fitness_tuples[-num_juniors:]]\n\n    new_population = population.copy()\n    dimensao = 60\n    f_unidimensionais = 16\n    jota = 355  # Starting index for unidimensional features\n\n    for i in range(len(population)):\n        senior_index = random.choice(senior_indices)\n        middle_index = random.choice(middle_indices)\n        junior_index = random.choice(junior_indices)\n\n        if fitness_tuples[i][1] > fitness_tuples[middle_index][1]:\n            for j in range(25):\n                if j < 6:\n                    start = j * dimensao\n                    end = start + dimensao\n                elif j > 21:\n                    start = (360 + f_unidimensionais) +(j-22)*dimensao\n                    end = start + dimensao\n                else:\n                    start = (jota + j) \n                    end = start+1  \n\n\n                new_population[i][start:end] = (\n                    new_population[i][start:end] +\n                    k_factor(kf) * (\n                        population[senior_index][start:end] - \n                        population[junior_index][start:end] + \n                        population[middle_index][start:end] - \n                        new_population[i][start:end]\n                    )\n                )\n                # Ensure the new solution values are between 0 and 1\n                new_population[i][start:end] = np.clip(new_population[i][start:end], 0, 1)\n            for j in range(25):\n                if j < 6:\n                    start = j * dimensao\n                    end = start + dimensao\n                elif j > 21:\n                    start = (360 + f_unidimensionais) +(j-22)*dimensao\n                    end = start + dimensao\n                else:\n                    start = (jota + j - 16) \n                    end = start+1\n\n                new_population[i][start:end] = (\n                    new_population[i][start:end] +\n                    k_factor(kf) * (\n                        population[senior_index][start:end] - \n                        population[junior_index][start:end] + \n                        new_population[i][start:end] - \n                        population[middle_index][start:end]\n                    )\n                )\n                new_population[i][start:end] = np.clip(new_population[i][start:end], 0, 1)\n\n    return new_population\n\n# Initialize the population\ndef initialize_population(pop_size, num_features=25, max_features=4):\n    dimensao = 60\n    size_total = 556#numero total de features\n    population = np.ones((pop_size, size_total))\n    f_unidimensionais =16\n    jota = 355 \n    for i in range(pop_size):\n        features_usadas = num_features\n        while features_usadas > max_features:\n            for k in range(num_features):\n                j = random.randint(0, 24)\n                if random.randint(0, 100) > 50:\n                    if j < 6:\n                        population[i][j * dimensao:(j * dimensao + dimensao)] = 0\n                        features_usadas -= 1\n                    if j >21:\n                        #features unidimensionais\n                        start = (j-f_unidimensionais)*dimensao + f_unidimensionais\n                        end = start + dimensao\n                        population[i][start:end] = 0\n                        features_usadas -= 1\n                    else:\n                        start =jota+j\n                        population[i][jota + j] = 0\n                        features_usadas -= 1\n                        \n                    # Stop the loop if features_usadas <= max_features\n                    if features_usadas <= max_features:\n                        break\n    return population\n\n\n# Re-evaluate fitness function\ndef revaluate_fitness(population, X, y):\n    fitness_values = [fitness_function(individual, X, y) for individual in population]\n    fitness_tuples = sorted(enumerate(fitness_values), key=lambda x: x[1])\n    return fitness_tuples\n\ndef population_reduction(population, fitness_tuples, generation_atual, generation_max,pop_size_inicial,low_b=0.12,high_b=0.6):\n    if int(pop_size_inicial*low_b) < len(population):\n        np_min =pop_size_inicial*low_b\n        np_max =pop_size_inicial*high_b\n        np_new = int((np_min - np_max) * (generation_atual / generation_max) + np_max -1)\n\n        for _ in range(len(population) - np_new):\n            remove_idx = int(fitness_tuples[-1][0])\n            population = np.delete(population, remove_idx, axis=0)\n            fitness_tuples = fitness_tuples[:-1]\n\n            # Update the remaining indices in fitness_tuples\n            fitness_tuples = [(idx if idx < remove_idx else idx - 1, fit) for idx, fit in fitness_tuples]\n            fitness_tuples.sort(key=lambda x: x[1])\n    \n    return population, fitness_tuples\n\n# Initialize feature tracking\n\nimport numpy as np\nimport plotly.graph_objs as go\n\ndef FS_pBGSK(X, y, pop_size=100, num_generations=50, num_jobs=-1, kf=0.999):\n    num_features = X.shape[1]\n    population = initialize_population(pop_size, num_features, round(num_features - 5))\n    fitness_tuples = revaluate_fitness(population, X, y)\n    best_fitness_overall = float('inf')\n    feature_usage = np.zeros(X.shape[1])\n    feature_weighted_fitness = np.zeros(X.shape[1])\n    #avalia√ß√£o inicial\n    for individual, fitness in zip(population, [fit[1] for fit in fitness_tuples]):\n        feature_usage += individual\n        feature_weighted_fitness += individual * fitness\n    \n    grafico_performace(feature_weighted_fitness)    \n    for generation in range(num_generations):\n        population = gaining_phase(population, fitness_tuples, kf)\n        population = senior_gaining_phase(population, fitness_tuples, kf)\n\n        best_index_g, best_fitness_g = fitness_tuples[0]\n        if best_fitness_overall > best_fitness_g:\n            best_fitness_overall = best_fitness_g\n            best_solution = population[best_index_g]\n            print(f\"New Best Overall {generation + 1}: Best Fitness = {-best_fitness_overall}, Best Solution = {best_solution}, Dimension = {best_solution.sum()}\")\n\n        population, fitness_tuples = population_reduction(population, fitness_tuples, generation + 1, num_generations, pop_size)\n        print(f\"Current population shape: {population.shape}\")\n\n    # Final evaluation to return the best solution overall\n    fitness_tuples = revaluate_fitness(population, X, y)\n    best_index, best_fitness = fitness_tuples[0]\n    best_solution = population[best_index]\n    print(f\"Checking best performance: {fitness_function(best_solution, X, y)}, Dimension = {best_solution.sum()}\")\n    # Normalize weighted fitness by feature usage\n    feature_weighted_fitness = np.divide(feature_weighted_fitness, feature_usage, out=np.zeros_like(feature_weighted_fitness), where=feature_usage != 0)\n\n    return best_solution, best_fitness_overall, feature_weighted_fitness\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.graph_objs as go\nbest_solution, best_fitness_overall, feature_weighted_fitness = FS_pBGSK(X_transformed, Y_transformed)\n\ngrafico_performace(feature_weighted_fitness)\n# Create an interactive plot for the single best solution\nselected_features = np.where(best_solution == 1)[0]\ntrace = go.Scatter(\n    x=selected_features,\n    y=np.ones_like(selected_features),  # All y-values are 1, as we are only showing selected features\n    mode='markers',\n    marker=dict(size=10, color='red'),\n    hoverinfo='x+text',\n    text=['Feature {}'.format(i) for i in selected_features]\n)\n\nlayout = go.Layout(\n    title='Selected Features in the Best Solution',\n    xaxis=dict(title='Feature Index'),\n    yaxis=dict(title='', showticklabels=False),  # Hide y-axis labels\n    hovermode='closest'\n)\n\nfig = go.Figure(data=[trace], layout=layout)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}